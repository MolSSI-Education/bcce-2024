{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"./assets/molssi_ai.png\"\n",
    "         alt=\"MolSSI-AI Logo\"\n",
    "         width=400 \n",
    "         height=250\n",
    "    />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "- Learn the basics of linear regression in the context of a practical cheminformatics problem\n",
    "- Become familiar with the exploratory data analysis (EDA) process and data featurization\n",
    "- Understand the concept of model evaluation and the importance of cross-validation\n",
    "- Build and evaluate a variety of machine learning models with `scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, let's make sure we have the necessary libraries ready for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T16:51:46.314577Z",
     "start_time": "2024-06-26T16:51:46.301095Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd                 # for data manipulation\n",
    "import seaborn as sns               # for data visualization\n",
    "import matplotlib.pyplot as plt     # for data visualization\n",
    "import numpy as np                  # for numerical operations\n",
    "\n",
    "from sklearn.linear_model import LinearRegression           # for creating a linear regression model\n",
    "from sklearn.dummy import DummyRegressor                    # for creating a base regressor to compare the model with\n",
    "from sklearn.model_selection import train_test_split        # for splitting the data into training and testing sets\n",
    "from sklearn.metrics import mean_squared_error, r2_score    # for evaluating the model\n",
    "from sklearn.preprocessing import StandardScaler            # for scaling the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>NOTE</b>\n",
    "    <br>\n",
    "    We have added comments to clarify the purpose of each imported library.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will be working on a **supervised learning** problem which involves both the featurized data and their corresponding labels. The goal is to predict the solubility of a molecule given its chemical structure using regression models. The dataset we will be using is the **Delaney's solubility dataset**. It contains the chemical structures of 1144 compounds along with their experimentally measured solubility in mol/L. We provide the preprocessed dataset in the [repository](https://github.com/MolSSI-Education/bcce-2024/tree/main/data) comma-separated value (CSV) format.\n",
    "\n",
    "Let's read the solubility data and take a look at a few samples in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T16:49:13.095479Z",
     "start_time": "2024-06-26T16:49:13.088117Z"
    }
   },
   "outputs": [],
   "source": [
    "# Path to the preprocessed data file\n",
    "data_path = \"./data/solubility-processed.csv\"\n",
    "\n",
    "# Read the data into a DataFrame\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains the following columns:\n",
    "- **MolLogP**: solubility values estimated by [Daylight CLOGP version 4.72](https://www.daylight.com/dayhtml/doc/clogp/index.html)\n",
    "- **MolWt**: molecular weight\n",
    "- **NumRotatableBonds**: number of rotatable bonds\n",
    "- **AromaticProportion**: the portion of heavy atoms that are in an aromatic ring\n",
    "- **logS**: the solubility of the molecule in mol/L measured at 25 $\\degree C$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sidebar: A Brief Note on Solubility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T16:49:44.121462Z",
     "start_time": "2024-06-26T16:49:44.025680Z"
    }
   },
   "source": [
    "The solubility of a molecule is a critical property in medicinal chemistry, drug discovery, agrochemistry as it determines the bioavailability of a drug.\n",
    "The **Partition coefficient** measures the propensity of a neutral compound to dissolve in a mixture of two immiscible solvents, often water and octanol. In simple terms, it determines how much of a solute dissolves in the water phase versus the organic portion.\n",
    "\n",
    "The partition coefficient, $P$, is defined as:\n",
    "\n",
    "$$\n",
    "P = \\frac{[Solute]_{\\text{organic}}}{[Solute]_{\\text{aqueous}}}     \n",
    "$$\n",
    "\n",
    "where $[Solute]_{\\text{organic}}$ and $[Solute]_{\\text{aqueous}}$ are the concentrations of the solute in the organic and aqueous phases, respectively.\n",
    "Instead of $P$, we often use the logarithm of the partition coefficient, $\\log P$, which is more convenient to work with. A negative value of $\\log P$ indicates that the solute is more soluble in water, while a positive value indicates that the solute is more soluble in the organic phase. When $\\log P$ is close to zero, the solute is equally soluble in both phases. Although $\\log P$ is a constant value, its magnitude is dependent on the measurement conditions and the choice of the organic solvent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> \n",
    "    <b>Exercise</b>\n",
    "    <br>\n",
    "    <br>\n",
    "    What does logP value of 1 mean?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "\n",
    "A $\\log P$ value of 1 indicates that the solute is 10 times more soluble in the organic phase than in the aqueous phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step after defining the problem and accessing the data is to perform exploratory data analysis (EDA). EDA is a crucial step in the data analysis process as it helps us understand the data, identify patterns, and detect anomalies. It involves summarizing the main characteristics of the data, often using visual methods. \n",
    "\n",
    "Loading the data into a pandas DataFrame provides a convenient way to perform EDA. Let's start by getting a high-level overview of the dataset\n",
    "using the ``info`` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T16:49:45.746932Z",
     "start_time": "2024-06-26T16:49:45.735372Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data is already preprocessed, the ``Non-Null Count`` for all feature columns should be equal to the total number of samples in the dataset, *i.e.,* 1144. All features are of numerical type ``float64``. The experimentally measured solubility in the last column ``logS`` is the target variable that we want to our model to be able to predict using the remaining properties as features.\n",
    "\n",
    "The ``describe`` function provides a statistical summary of all numerical features in our dataset. The statistics provide the count, mean, standard deviation, minimum, 25th percentile, median, 75th percentile, and maximum values for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This information is extremely useful for understanding the data and the distribution of the features. It helps in identifying any missing values or anomalies in the data or if our data requires any preprocessing. For example, the skeweness of the data which is measured by comparing the mean and median values can be visualized using Pandas's ``hist`` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(figsize=(8,8), edgecolor='black', grid=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the solubility values are normally distributed while other features are more or less positively or negatively skewed. The Pandas library helps us quantify these observations for each feature using the ``skew`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T16:50:52.798960Z",
     "start_time": "2024-06-26T16:50:52.792664Z"
    }
   },
   "outputs": [],
   "source": [
    "df.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numerical values of skewness can be interpreted as follows:\n",
    "- The skewness value of zero indicates a perfect symmetrical distribution,\n",
    "- a skewness between -0.5 and 0.5 indicates an approximately symmetric distribution,\n",
    "- a skewness between -1 and -0.5 (or 0.5 and 1) indicates a moderately skewed distribution,\n",
    "- a skewness between -1.5 and -1 (or 1 and 1.5) indicates a highly skewed distribution, and\n",
    "- a skewness less than -1.5 (or greater than 1.5) indicates an extremely skewed distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a good understanding of the data, we can move on to the next step, which is feature engineering. Feature engineering is the process of transforming raw data into a format that is suitable for machine learning models. It involves creating new features, selecting relevant features, and transforming existing features to improve the model's performance.\n",
    "\n",
    "Here, we use Pandas's ``corr`` function to calculate the correlation matrix between the features and the target variable. The correlation matrix provides insights into the relationships between the features and the target variable. A correlation value close to 1 indicates a strong positive relationship, while a correlation value close to -1 indicates a strong negative relationship. A correlation value close to 0 indicates no relationship between the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T16:50:49.364485Z",
     "start_time": "2024-06-26T16:50:49.350517Z"
    }
   },
   "outputs": [],
   "source": [
    "df.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It maybe eaiser to see the relationship between the features and the target variable using a heatmap correlation matrix plot. We can use the Seaborn library to create a heatmap plot of the correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T16:50:51.090312Z",
     "start_time": "2024-06-26T16:50:50.767327Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a plot object\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr = df.corr()\n",
    "\n",
    "# Create a heatmap\n",
    "sns.set(font_scale=1.1)\n",
    "sns.heatmap(corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "\n",
    "# Display the heatmap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our coloring scheme makes it easy to uncover the relationships between the features. The darker the color, the stronger the correlation. The diagonal line represents the correlation of each feature with itself, which is always 1. The red color indicates a positive correlation, while the blue color indicates a negative correlation.\n",
    "\n",
    "From the correlation matrix, we can see that ``MolLogP`` shows a strong negative correlation with the target variable ``logS``. This is expected as the CLOGP model estimates the solubility of a molecule from its chemical structure. The remaining features, ``MolWt``, ``AromaticProportion``, and ``NumRotatableBonds``show weaker correlations with the target variable, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step in the feature engineering process is to split the data into features and target variables. We will use the ``train_test_split`` function from the ``sklearn.model_selection`` module to split the data into training and testing sets. The training set will be used to train the model, while the testing set will be used to evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the feature matrix (X), feature vector (x), and the target vector (y)\n",
    "X = df.drop(columns=['logS'])\n",
    "x = X[\"MolLogP\"]\n",
    "y = df['logS']\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=123, shuffle=True)\n",
    "\n",
    "# Reshape the data into 2D arrays of shape (n_samples, n_features)\n",
    "x_train = x_train.values.reshape(-1,1)\n",
    "x_test = x_test.values.reshape(-1,1)\n",
    "y_train = y_train.values.reshape(-1,1)\n",
    "y_test = y_test.values.reshape(-1,1)\n",
    "\n",
    "# Display the shapes of the training and testing sets\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Training a Linear Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step after the data preparation is to build and train our linear regression model. First we will build a simple linear regression model which focuses on the relationship between a single feature (``MolLogP``) and the target variable (``logS``). \n",
    "\n",
    "In order to be able to compare the performance of our models, we create a baseline model using the ``DummyRegressor`` class from the ``sklearn.dummy`` module. This class provides a simple way to create a model that predicts the mean value of the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T16:51:57.478588Z",
     "start_time": "2024-06-26T16:51:57.469581Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a baseline model using the mean value of the target variable\n",
    "base_model = DummyRegressor(strategy=\"mean\")\n",
    "\n",
    "# Fit the model to the training data\n",
    "base_model.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred_base = base_model.predict(x_test)\n",
    "\n",
    "# Calculate the performance metrics and store them in a DataFrame\n",
    "results = pd.DataFrame({\n",
    "    \"Coefficients\": [np.array(base_model.constant_)],         # the regression coefficient\n",
    "    \"MSE\": mean_squared_error(y_test, y_pred_base),     # the mean squared error\n",
    "    \"R2\": r2_score(y_test, y_pred_base)                 # the coefficient of determination\n",
    "}, index=[\"Baseline\"])\n",
    "                            \n",
    "\n",
    "# Set the formatting style\n",
    "results.style.format(\n",
    "    {\n",
    "        \"MSE\": \"{:.3f}\",\n",
    "        \"R2\": \"{:.2f}\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build and train our single-feature linear regression model. We will use the ``LinearRegression`` class from the ``sklearn.linear_model`` module to create the model. The ``fit`` method is used to train the model on the training data. Once the model is trained, we can use the ``predict`` method to make predictions on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple linear regression model\n",
    "simple_reg_model = LinearRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "simple_reg_model.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_simple = simple_reg_model.predict(x_test)\n",
    "\n",
    "# Calculate the performance metrics\n",
    "simple_model_results = pd.DataFrame({\n",
    "    \"Coefficients\": [np.array(simple_reg_model.coef_)],    # the regression coefficient\n",
    "    \"MSE\": mean_squared_error(y_test, y_pred_simple),     # the mean squared error\n",
    "    \"R2\": r2_score(y_test, y_pred_simple)                 # the coefficient of determination\n",
    "}, index=[\"Simple-Linear-Regression\"])\n",
    "\n",
    "# Store the results into results DataFrame\n",
    "results = pd.concat([results, simple_model_results])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the table above, we can see our simple linear regression model using the ``MolLogP`` feature has a mean squared error (MSE) of 1.05 and a coefficient of determination ($R^2$) of 0.66. We can visualize the performance of our fitted models using ``matplotlib`` as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T16:52:00.069272Z",
     "start_time": "2024-06-26T16:51:59.817331Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a plot object\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "# Plot the test data\n",
    "ax.scatter(x_test, y_test, color='blue', label='Test Data')\n",
    "\n",
    "# Plot the simple linear regression model\n",
    "ax.plot(x_test, y_pred_simple, color='red', label='Simple Linear Regression')\n",
    "\n",
    "# Plot the baseline model\n",
    "ax.plot(x_test, y_pred_base, \"g--\", label=\"Baseline\")\n",
    "\n",
    "# Create the legends\n",
    "fig.legend(facecolor='white')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the ``MolLogP`` feature with highest correlation with the target variable ``logS`` is a good starting point. The reason is that we knew beforehand that the CLOGP estimations would be a great predictor of solubility. However, we can improve our model by including other features that show some correlation with the target variable. This is where multiple linear regression comes in.\n",
    "\n",
    "We will build a multiple linear regression model using all the features in our dataset and the process is similar to building a single-feature linear regression model. The ``fit`` method is used to train the model on the training data, and the ``predict`` method is used to make predictions on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T16:52:01.901353Z",
     "start_time": "2024-06-26T16:52:01.889727Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123, shuffle=True)\n",
    "\n",
    "# Create a linear regression model\n",
    "multi_feature_model = LinearRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "multi_feature_model.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_multi_feature = multi_feature_model.predict(x_test)\n",
    "\n",
    "\n",
    "# Calculate the performance metrics\n",
    "multi_feature_model_results = pd.DataFrame({\n",
    "    \"Coefficients\": [np.array(multi_feature_model.coef_)],       # the regression coefficient\n",
    "    \"MSE\": mean_squared_error(y_test, y_pred_multi_feature),     # the mean squared error\n",
    "    \"R2\": r2_score(y_test, y_pred_multi_feature)                 # the coefficient of determination\n",
    "}, index=[\"Multi-Linear-Regression\"])\n",
    "\n",
    "# Store the results into results DataFrame\n",
    "results = pd.concat([results, multi_feature_model_results])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our multiple linear regression with a mean squared error (MSE) of 1.13 and a coefficient of determination ($R^2$) of 0.75 improves upon its single-feature counterpart with MSE = 1.53 and $R^2 = 0.66$. These results are not surprising as the multiple linear regression model can benefit from the additional information provided by the other features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>NOTE</b>\n",
    "    <br>\n",
    "    <br>\n",
    "    Note the usage of <b>X</b> matrix instead of <b>x</b> vector in the <b>train_test_split</b> function.\n",
    "    This is because we are using multiple features to predict the target variable.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better Model Evaluation with Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate the performance of our trained model on the test data, we have split the data into training and testing sets. The performance of the model may vary depending on how the quality of data in the splits. Furthermore if the models has hyperparameters to be fine-tuned, the performance may be overestimated due to the tuning on the test set and the **data leakage** effect.\n",
    "\n",
    "In order to ameliorate this issue, we can use a technique called the ``cross_val_score`` function from the ``sklearn.model_selection`` module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "# Create a KFold object\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_results = cross_val_score(multi_feature_model, X, y, cv=kf, scoring=\"neg_mean_squared_error\")\n",
    "\n",
    "# Calculate the mean and standard deviation of the cross-validation results\n",
    "print(f\"CV Results Mean: {cv_results.mean():.5f}\")\n",
    "print(f\"CV Results STD: {cv_results.std():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> \n",
    "    <b>Exercise</b>\n",
    "    <br>\n",
    "    <br>\n",
    "    Perform a 5-fold cross-validation on the multiple linear regression model and compare the results with the previous model evaluation\n",
    "    based on the R^2 metric.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "```python\n",
    "\n",
    "    from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "    # Create a KFold object\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "\n",
    "    # Perform cross-validation\n",
    "    cv_results = cross_val_score(multi_feature_model, X, y, cv=kf, scoring=\"r2\")\n",
    "\n",
    "    # Calculate the mean and standard deviation of the cross-validation results\n",
    "    print(f\"CV Results Mean: {cv_results.mean():.5f}\")\n",
    "    print(f\"CV Results STD: {cv_results.std():.5f}\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Section: Automation with PyCaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.regression import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = setup(data = df, target = 'logS', session_id=123, fold=5, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the list of options you have for the ``plot_model`` function:\n",
    "    \n",
    "* ``pipeline`` - Schematic drawing of the preprocessing pipeline\n",
    "* ``residuals_interactive`` - Interactive Residual plots\n",
    "* ``residuals`` - Residuals Plot\n",
    "* ``error`` - Prediction Error Plot\n",
    "* ``cooks`` - Cooks Distance Plot\n",
    "* ``rfe`` - Recursive Feat. Selection\n",
    "* ``learning`` - Learning Curve\n",
    "* ``vc`` - Validation Curve\n",
    "* ``manifold`` - Manifold Learning\n",
    "* ``feature`` - Feature Importance\n",
    "* ``feature_all`` - Feature Importance (All)\n",
    "* ``parameter`` - Model Hyperparameter\n",
    "* ``tree`` - Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(best, plot='vc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(best, plot='feature')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
